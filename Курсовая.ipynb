{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyMqKirqYWRlOCbvuL3L4ZuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igoryan75/PI/blob/main/%D0%9A%D1%83%D1%80%D1%81%D0%BE%D0%B2%D0%B0%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFv_ZYSF_VgM"
      },
      "outputs": [],
      "source": [
        " !pip install naeval;\n",
        " !pip install nerus;\n",
        " !pip install corus;\n",
        " !pip install transformers;\n",
        " !pip install transformers[torch];\n",
        " !pip install seqeval;\n",
        " !pip install tokenizers;\n",
        " !pip install datasets;\n",
        " !pip install torch;\n",
        " !pip install accelerate -U;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/dialogue-evaluation/factRuEval-2016/archive/master.zip;\n",
        "!unzip master.zip;\n",
        "!rm master.zip;"
      ],
      "metadata": {
        "collapsed": true,
        "id": "27N1cQEE_Y0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NsqX-oN8Vpp",
        "outputId": "8b1ed9f6-2586-443b-9227-1bcd566a6181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r pdn_model_ariesjr\n",
        "!rm -r train_result_ariesjr\n",
        "!rm -r factRuEval-2016-master"
      ],
      "metadata": {
        "id": "8NSo5r0sOPxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели\n"
      ],
      "metadata": {
        "id": "ROdddpvz8w_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.scheme import IOB2\n",
        "from corus import load_factru\n",
        "\n",
        "\n",
        "records = list(load_factru(\"factRuEval-2016-master\"))\n",
        "model_name = \"DeepPavlov/rubert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_tok_len = 512\n",
        "\n",
        "\n",
        "types = [\"SURNAME\", \"NAME\"]\n",
        "label_names = [\"O\"] + [f\"{prefix}-{t}\" for t in types for prefix in [\"B\", \"I\"]]\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# --- Токенизация и разметка ---\n",
        "result = {'input_ids': [], 'token_type_ids': [], 'attention_mask': [], 'labels': []}\n",
        "\n",
        "for rec in tqdm(records):\n",
        "    tokenized_inputs = tokenizer(rec.text, truncation=True, max_length=max_tok_len, padding=True)\n",
        "    enc = tokenized_inputs.encodings[0]\n",
        "    offsets = enc.offsets\n",
        "    word_ids = enc.word_ids\n",
        "\n",
        "    labels = []\n",
        "    temp_tags = []\n",
        "\n",
        "    for i in range(len(offsets)):\n",
        "        if word_ids[i] is None:\n",
        "            labels.append(-100)\n",
        "            temp_tags.append(\"O\")\n",
        "            continue\n",
        "\n",
        "        # Проверяем, мульти-токен одного слова\n",
        "        if i > 0 and word_ids[i-1] == word_ids[i]:\n",
        "            labels.append(-100)\n",
        "            temp_tags.append(temp_tags[-1])\n",
        "            continue\n",
        "\n",
        "        offset = offsets[i]\n",
        "        tag_found = \"O\"\n",
        "\n",
        "        # Ищем принадлежность span-у\n",
        "        for obj in rec.objects:\n",
        "            for span in obj.spans:\n",
        "                # Берём только SURNAME и NAME\n",
        "                if span.type.upper() not in types:\n",
        "                    continue\n",
        "\n",
        "                if span.start <= offset[0] < span.stop or span.start < offset[1] <= span.stop:\n",
        "                    prefix = \"I-\" if temp_tags and temp_tags[-1] == span.type else \"B-\"\n",
        "                    tag_found = prefix + span.type.upper()\n",
        "                    break\n",
        "            if tag_found != \"O\":\n",
        "                break\n",
        "\n",
        "        labels.append(label2id.get(tag_found, 0))\n",
        "        temp_tags.append(tag_found.replace(\"B-\", \"\").replace(\"I-\", \"\") if tag_found != \"O\" else \"O\")\n",
        "\n",
        "    result['input_ids'].append(tokenized_inputs['input_ids'])\n",
        "    result['token_type_ids'].append(tokenized_inputs['token_type_ids'])\n",
        "    result['attention_mask'].append(tokenized_inputs['attention_mask'])\n",
        "    result['labels'].append(labels)\n",
        "\n",
        "# --- Train/Test/Valid ---\n",
        "ds = Dataset.from_dict(result)\n",
        "gen = np.random.default_rng(42)\n",
        "dsd = ds.train_test_split(test_size=0.2, generator=gen)\n",
        "dsd_temp = dsd[\"test\"].train_test_split(test_size=0.5, generator=gen)\n",
        "\n",
        "dsd = DatasetDict({\n",
        "    \"train\": dsd[\"train\"],\n",
        "    \"test\": dsd_temp[\"test\"],\n",
        "    \"valid\": dsd_temp[\"train\"]\n",
        "})\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_names),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    y_true = [[id2label[l] for l, p in zip(label, pred) if l != -100] for label, pred in zip(labels, predictions)]\n",
        "    y_pred = [[id2label[p] for l, p in zip(label, pred) if l != -100] for label, pred in zip(labels, predictions)]\n",
        "\n",
        "    res = classification_report(y_true=y_true, y_pred=y_pred, scheme=IOB2, output_dict=True)[\"weighted avg\"]\n",
        "    res.pop(\"support\", None)\n",
        "    return res\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./train_result_name_surname\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=8,\n",
        "    logging_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        "    save_safetensors=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dsd[\"train\"],\n",
        "    eval_dataset=dsd[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Обучение и сохранение модели ---\n",
        "trainer.train()\n",
        "model_dir = \"./pdn_model_name_surname\"\n",
        "trainer.save_model(model_dir)\n",
        "print(f\"Модель сохранена в {model_dir}\")\n"
      ],
      "metadata": {
        "id": "yV_ejHzbAdDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск разспознавания ПДн"
      ],
      "metadata": {
        "id": "naaoksqi8syL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/_General.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/CADnum.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/CARD.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/INN.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/Married.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/namesurname.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/PASS.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/Phone.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/SNILS.txt -P tests/\n",
        "!wget https://raw.githubusercontent.com/igoryan75/PI/main/tests/URL.txt -P tests/\n",
        "!cd tests && ls;"
      ],
      "metadata": {
        "id": "0V0_VAVTzMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import os\n",
        "\n",
        "tests_dir = \"tests\"  # папка с тестовыми файлами\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_dir = \"./pdn_model_name_surname\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "tokenizer.model_max_length = 512\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_dir, local_files_only=True).to(device)\n",
        "id2label = model.config.id2label\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#  ВАЛИДАЦИИ\n",
        "# ============================================================\n",
        "\n",
        "def check_luhn(number: str) -> bool:\n",
        "    digits = [int(d) for d in number if d.isdigit()]\n",
        "    if len(digits) != 16:\n",
        "        return False\n",
        "\n",
        "    checksum = 0\n",
        "    parity = len(digits) % 2\n",
        "\n",
        "    for i, digit in enumerate(digits):\n",
        "        if i % 2 == parity:\n",
        "            d = digit * 2\n",
        "            if d > 9:\n",
        "                d -= 9\n",
        "        else:\n",
        "            d = digit\n",
        "        checksum += d\n",
        "\n",
        "    return checksum % 10 == 0\n",
        "\n",
        "\n",
        "def check_inn(inn: str) -> bool:\n",
        "    inn = re.sub(r'\\D', '', inn)\n",
        "    if len(inn) == 10:\n",
        "        nums = list(map(int, inn))\n",
        "        coeffs = [2,4,10,3,5,9,4,6,8]\n",
        "        s = sum(a*b for a, b in zip(nums, coeffs)) % 11 % 10\n",
        "        return s == nums[9]\n",
        "\n",
        "    if len(inn) == 12:\n",
        "        nums = list(map(int, inn))\n",
        "        coeffs1 = [7,2,4,10,3,5,9,4,6,8,0]\n",
        "        c1 = sum(a*b for a, b in zip(nums, coeffs1)) % 11 % 10\n",
        "        coeffs2 = [3,7,2,4,10,3,5,9,4,6,8]\n",
        "        c2 = sum(a*b for a, b in zip(nums, coeffs2)) % 11 % 10\n",
        "        return c1 == nums[10] and c2 == nums[11]\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def check_snils(snils: str) -> bool:\n",
        "    digits = re.sub(r'\\D', '', snils)\n",
        "    if len(digits) != 11:\n",
        "        return False\n",
        "\n",
        "    nums = list(map(int, digits))\n",
        "    control = nums[-2] * 10 + nums[-1]\n",
        "    s = sum((9 - i) * nums[i] for i in range(9))\n",
        "\n",
        "    if s < 100:\n",
        "        return s == control\n",
        "    elif s in (100, 101):\n",
        "        return control == 0\n",
        "    else:\n",
        "        return (s % 101) == control\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#  РЕГУЛЯРКИ ДЛЯ ПДН\n",
        "# ============================================================\n",
        "\n",
        "regex_rules = [\n",
        "    (\"CARD\", re.compile(r\"\\b(?:\\d{4}[ -]?){3}\\d{4}\\b\")),\n",
        "    (\"INN\", re.compile(r\"\\b\\d{10}\\b|\\b\\d{12}\\b\")),\n",
        "    (\"PASSPORT\", re.compile(r\"\\b\\d{4}\\s\\d{6}\\b\")),\n",
        "    (\"SNILS\", re.compile(r\"\\b\\d{3}-\\d{3}-\\d{3}\\s?\\d{2}\\b\")),\n",
        "    (\"URL\", re.compile(r\"\\b((?:https?://)?(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?)\\b\")),\n",
        "    (\"PHONE\", re.compile(r\"(?:(?:\\+7|8)[\\s\\-]?\\(?\\d{3}\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{2}[\\s\\-]?\\d{2})\")),\n",
        "    (\"MARRIAGE_CERT\", re.compile(r\"\\b\\d{2}-\\d{2}-\\d{6}\\b\")),\n",
        "    (\"CAD_NUMBER\", re.compile(r\"\\b\\d{2}:\\d{2}:\\d{6,7}:\\d+\\b\")),\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#  РАСПОЗНАВАНИЕ МОДЕЛЬЮ (ФИ + ИМЯ)\n",
        "# ============================================================\n",
        "\n",
        "def recognize_pdn_model(text):\n",
        "    toks = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
        "    input_ids = torch.tensor([toks[\"input_ids\"]]).to(device)\n",
        "    offsets = toks[\"offset_mapping\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
        "\n",
        "    result = []\n",
        "    curr_tag = None\n",
        "    curr_start = None\n",
        "    curr_end = None\n",
        "\n",
        "    for p, (start, end) in zip(preds, offsets):\n",
        "        tag_full = id2label.get(int(p), \"O\")\n",
        "        tag = tag_full.split(\"-\")[-1] if tag_full != \"O\" else \"O\"\n",
        "\n",
        "        if tag not in (\"SURNAME\", \"NAME\"):\n",
        "            if curr_tag:\n",
        "                result.append([curr_tag, curr_start, curr_end])\n",
        "                curr_tag = None\n",
        "            continue\n",
        "\n",
        "        if tag != curr_tag:\n",
        "            if curr_tag:\n",
        "                result.append([curr_tag, curr_start, curr_end])\n",
        "            curr_tag = tag\n",
        "            curr_start, curr_end = start, end\n",
        "        else:\n",
        "            curr_end = end\n",
        "\n",
        "    if curr_tag:\n",
        "        result.append([curr_tag, curr_start, curr_end])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#  (regex + BERT)\n",
        "# ============================================================\n",
        "\n",
        "if not os.path.exists(tests_dir):\n",
        "    raise FileNotFoundError(f\"Папка {tests_dir} не найдена!\")\n",
        "if not os.path.isdir(tests_dir):\n",
        "    raise NotADirectoryError(f\"{tests_dir} существует, но это не папка!\")\n",
        "\n",
        "for filename in os.listdir(tests_dir):\n",
        "    if not filename.lower().endswith(\".txt\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(tests_dir, filename)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    print(f\"\\n===== Файл: {filename} =====\")\n",
        "    valid = []\n",
        "    invalid = []\n",
        "\n",
        "    # regex + проверки\n",
        "    for label, pattern in regex_rules:\n",
        "        for m in pattern.finditer(text):\n",
        "            s, e = m.span()\n",
        "            val = m.group()\n",
        "            is_valid = True\n",
        "\n",
        "            if label == \"CARD\" and not check_luhn(val):\n",
        "                is_valid = False\n",
        "            if label == \"INN\" and not check_inn(val):\n",
        "                is_valid = False\n",
        "            if label == \"SNILS\" and not check_snils(val):\n",
        "                is_valid = False\n",
        "\n",
        "            if is_valid:\n",
        "                valid.append([label, s, e])\n",
        "            else:\n",
        "                invalid.append([label, s, e])\n",
        "\n",
        "    # BERT (SURNAME, NAME)\n",
        "    for tag, s, e in recognize_pdn_model(text):\n",
        "        valid.append([tag, s, e])\n",
        "\n",
        "    # вывод\n",
        "    print(\"\\n--- ПДН прошедшие валидацию ---\")\n",
        "    for r in valid:\n",
        "        print(r)\n",
        "\n",
        "    print(\"\\n--- ПДН не прошедшие валидацию ---\")\n",
        "    for r in invalid:\n",
        "        print(r)\n"
      ],
      "metadata": {
        "id": "ArJouPiuY_vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff05a967-5068-42ba-d714-f97705de552d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Файл: namesurname.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['NAME', 18, 22]\n",
            "['SURNAME', 23, 29]\n",
            "['NAME', 31, 36]\n",
            "['NAME', 37, 45]\n",
            "['SURNAME', 46, 55]\n",
            "['NAME', 57, 62]\n",
            "['SURNAME', 63, 71]\n",
            "['SURNAME', 72, 80]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "\n",
            "===== Файл: INN.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['INN', 21, 33]\n",
            "['MARRIAGE_CERT', 64, 76]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['INN', 48, 58]\n",
            "\n",
            "===== Файл: CARD.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['CARD', 36, 55]\n",
            "['CARD', 71, 87]\n",
            "['CARD', 106, 125]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['CARD', 145, 164]\n",
            "\n",
            "===== Файл: URL.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['URL', 22, 41]\n",
            "['URL', 55, 74]\n",
            "['URL', 86, 116]\n",
            "['URL', 138, 153]\n",
            "['URL', 154, 164]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "\n",
            "===== Файл: CADnum.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['CAD_NUMBER', 39, 56]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "\n",
            "===== Файл: _General.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['CARD', 155, 174]\n",
            "['INN', 38, 50]\n",
            "['PASSPORT', 60, 71]\n",
            "['PASSPORT', 310, 321]\n",
            "['SNILS', 79, 93]\n",
            "['URL', 100, 119]\n",
            "['URL', 350, 369]\n",
            "['PHONE', 129, 147]\n",
            "['PHONE', 379, 397]\n",
            "['MARRIAGE_CERT', 198, 210]\n",
            "['MARRIAGE_CERT', 448, 460]\n",
            "['CAD_NUMBER', 220, 237]\n",
            "['CAD_NUMBER', 470, 487]\n",
            "['NAME', 21, 25]\n",
            "['SURNAME', 26, 32]\n",
            "['NAME', 271, 275]\n",
            "['SURNAME', 276, 282]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['CARD', 405, 424]\n",
            "['INN', 288, 300]\n",
            "['SNILS', 329, 343]\n",
            "\n",
            "===== Файл: SNILS.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['SNILS', 25, 39]\n",
            "['SNILS', 91, 104]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['SNILS', 56, 70]\n",
            "\n",
            "===== Файл: Phone.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['PHONE', 18, 34]\n",
            "['PHONE', 44, 61]\n",
            "['PHONE', 71, 82]\n",
            "['PHONE', 92, 108]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "\n",
            "===== Файл: PASS.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['PASSPORT', 30, 41]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['INN', 63, 73]\n",
            "\n",
            "===== Файл: Married.txt =====\n",
            "\n",
            "--- ПДН прошедшие валидацию ---\n",
            "['MARRIAGE_CERT', 49, 61]\n",
            "\n",
            "--- ПДН не прошедшие валидацию ---\n",
            "['INN', 72, 82]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ToMmHdtVyGjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}